{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73eecad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from glob import glob\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from scipy.signal import welch, butter, lfilter\n",
    "\n",
    "from preprocess_eeg_signal import butter_bandpass, bandpass_filter, band_power_envelope, multiband_features \n",
    "\n",
    "load_dotenv()\n",
    "root_dir = os.getenv(\"ROOT_DIR\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efede0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class InnerSpeechDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "770e42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class EEGInnerSpeechDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8228582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EEGInnerSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_bands=4, num_channels=128):\n",
    "        super(EEGInnerSpeechClassifier, self).__init__()\n",
    "        # Conv1: Process channels across bands\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_bands, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Conv2: Separable convolution (depthwise + pointwise)\n",
    "        self.conv2_depth = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2, groups=32)\n",
    "        self.conv2_point = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Conv3: Standard convolution\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(128 * 16, 256)  # 128 channels * 16 (after pooling)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (batch, 4, 128)\n",
    "        x = self.conv1(x)  # (batch, 32, 128)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)  # (batch, 32, 64)\n",
    "        x = self.conv2_depth(x)  # (batch, 32, 64)\n",
    "        x = self.conv2_point(x)  # (batch, 64, 64)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)  # (batch, 64, 32)\n",
    "        x = self.conv3(x)  # (batch, 128, 32)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)  # (batch, 128, 16)k\n",
    "        x = x.view(x.size(0), -1)  # (batch, 128*16)\n",
    "        x = self.relu4(self.fc1(x))  # (batch, 256)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # (batch, 4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d41bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, val_loader=None, epochs=50, batch_size=16, model_name=\"EEGInnerSpeechCNN\", example_input=None, checkpoint_dir=\"models/\", verbose=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    os.makedirs(\"models/checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"models/runs\", exist_ok=True)\n",
    "    GRAD_CLIP = 1.0\n",
    "    patience = 5  # epochs\n",
    "    warmup_epochs = 3\n",
    "    initial_lr = 1e-5\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model_optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(model_optimizer, mode=\"min\", factor=0.5, patience=patience, min_lr=1e-5)\n",
    "    writer = SummaryWriter(log_dir='runs/' + model_name)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    if example_input is not None:\n",
    "        writer.add_graph(model, example_input.to(device))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Leanring rate warmup\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = initial_lr + (5e-4 - initial_lr) * (epoch / warmup_epochs)\n",
    "            for param_group in model_optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).squeeze().long()\n",
    "            if verbose: print(f\"y_batch.shape: {y_batch.shape}\")\n",
    "            model_optimizer.zero_grad()\n",
    "            logits = model(X_batch)  # logits, shape (batch, 4)\n",
    "            if verbose: print(f\"logits.shape: {logits.shape}\")\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            \n",
    "            model_optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            predicted_classes = logits.argmax(dim=1)\n",
    "            all_train_preds.append(predicted_classes.cpu())\n",
    "            all_train_targets.append(y_batch.cpu())\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_preds = torch.cat(all_train_preds).numpy()\n",
    "        train_targets = torch.cat(all_train_targets).numpy()\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", model_optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_val_preds = []\n",
    "            all_val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device).squeeze().long()\n",
    "                    if verbose: print(f\"y_batch.shape: {y_batch.shape}\")\n",
    "                    logits = model(X_batch)\n",
    "                    if verbose: print(f\"logits.shape: {logits.shape}\")\n",
    "                    loss = criterion(logits, y_batch)\n",
    "                    running_val_loss += loss.item() * X_batch.size(0)\n",
    "                    all_val_preds.append(logits.argmax(dim=1).cpu())\n",
    "                    all_val_targets.append(y_batch.cpu())\n",
    "            \n",
    "            avg_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            val_preds = torch.cat(all_val_preds).numpy()\n",
    "            val_targets = torch.cat(all_val_targets).numpy()\n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "            \n",
    "            writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "            if epoch >= warmup_epochs:\n",
    "                scheduler.step(avg_val_loss)\n",
    "\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.6f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.6f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if avg_val_loss < best_val_loss - 1e-5:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stop_counter = 0\n",
    "                print(f\"Model Checkpoint | epoch: {epoch} | best_val_loss: {best_val_loss}\")\n",
    "                torch.save(model.state_dict(), checkpoint_dir + model_name + \".pth\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96351a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Neural Network Model\n",
    "# --- CNN Model ---\n",
    "class EcogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Down to (256, 576)\n",
    "            nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Down to (128, 288)\n",
    "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (64, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),      # (64,)\n",
    "            nn.Linear(64, 4)   # 3 output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.net(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scale_channels_fit(X, scaler_name_prefix=None):\n",
    "#     \"\"\"\n",
    "#     This is used for precise per-channel scaling.\n",
    "#     This will improve model performance.\n",
    "\n",
    "#     Args:\n",
    "#         X (numpy.ndarray): shape (3000, 128, 1153)\n",
    "#         scaler_name_prefix (str): prefix to identify the scaler.\n",
    "\n",
    "#     Returns:\n",
    "#         numpy.ndarray: X_scaled training data of zero-mean & unit variance.\n",
    "#     \"\"\"\n",
    "#     n_samples, n_channels, n_timespoints = X.shape\n",
    "#     X_scaled = np.zeros_like(X)\n",
    "#     scales = []\n",
    "#     for channel in range(n_channels):\n",
    "#         scaler = StandardScaler()\n",
    "#         X_ch = X[:, channel, :]\n",
    "#         X_scaled[:, channel, :] = scaler.fit_transform(X_ch)\n",
    "#         scales.append(scaler)\n",
    "#     if scaler_name_prefix is not None: \n",
    "#         joblib.dump(scales, scaler_name_prefix + \"_scales.pkl\")\n",
    "#     return X_scaled\n",
    "\n",
    "# def scale_channels_transform(X, scaler_name_prefix = None):\n",
    "#     \"\"\"This is used to scale X test with the same scaler that X_train was scaled with.\n",
    "\n",
    "#     Args:\n",
    "#         X (numpy.ndarray): shape (1000, 128, 1153)\n",
    "#         scaler_name_prefix (str): prefix to identify the scaler.\n",
    "\n",
    "#     Returns:\n",
    "#         numpy.ndarray: X_scaled test data of zero-mean & unit variance.\n",
    "#     \"\"\"\n",
    "#     if scaler_name_prefix is not None:\n",
    "#         scalers = joblib.load(f\"{scaler_name_prefix}_scales.pkl\")\n",
    "#     n_samples, n_channels, n_timespoints = X.shape\n",
    "#     X_scaled = np.zeros_like(X)\n",
    "#     for channel in range(n_channels):\n",
    "#         X_ch = X[:, channel, :]\n",
    "#         X_scaled[:, channel, :] = scalers[channel].transform(X_ch)\n",
    "#     return X_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db194545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf3f8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data, lowcut, highcut, fs=256.0):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs)\n",
    "    filtered = lfilter(b, a, data, axis=2)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3339cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(data, fs=256.0, bands=[(0.5, 4), (4, 8), (8, 12), (12, 30)], batch_size=100):\n",
    "    num_trials, num_channels, num_samples = data.shape\n",
    "    num_bands = len(bands)\n",
    "    psd_features = np.zeros((num_trials, num_bands, num_channels), dtype=np.float32)\n",
    "\n",
    "    for start_idx in range(0, num_trials, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_trials)\n",
    "        batch_data = data[start_idx:end_idx]\n",
    "\n",
    "        for band_idx, (low, high) in enumerate(bands):\n",
    "            filtered = bandpass_filter(batch_data, low, high, fs)\n",
    "            freqs, psd = welch(filtered, fs=fs, axis=2, nperseg=256)\n",
    "            psd_mean = np.mean(psd, axis=2)\n",
    "            psd_features[start_idx:end_idx, band_idx, :] = psd_mean\n",
    "            del filtered, psd, psd_mean\n",
    "    \n",
    "        del batch_data\n",
    "    return psd_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d825da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_channels_fit(X, name):\n",
    "    scaler = RobustScaler()\n",
    "    X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "    scaler.fit(X_reshaped)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "635ffd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_channels_transform(X, scaler):\n",
    "    X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "    X_scaled = scaler.transform(X_reshaped)\n",
    "    return X_scaled.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "691fdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGInnerSpeechClassifierUpdated(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_bands=4):\n",
    "        super(EEGInnerSpeechClassifierUpdated, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(num_bands, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.depthwise = nn.Conv1d(32, 32, kernel_size=5, padding=2, groups=32)\n",
    "        self.pointwise = nn.Conv1d(32, 64, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.pointwise(self.depthwise(x)))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a587fc6",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68ee7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(\"data/X_train.pth\")\n",
    "y_train = torch.load(\"data/y_train.pth\")\n",
    "# X_test = torch.load(\"data/X_test.pth\")\n",
    "# y_test = torch.load(\"data/y_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "decc3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "# y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35d7d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "y_train = y_train.cpu().numpy()\n",
    "X_train = X_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "221b806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = X_test.cpu().numpy()\n",
    "# y_test = y_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19eabd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 256.0\n",
    "X_train_psd = compute_psd(X_train, fs=fs, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c400dde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = scale_channels_fit(X_train_psd, \"X_train\")\n",
    "X_train_scaled = scale_channels_transform(X_train_psd, scaler)\n",
    "del X_train, \n",
    "# X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "650f3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1)\n",
    "assert y_train.shape[0] == X_train_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "287beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_val_split, y_train_split, y_validation_split = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4e36d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dda976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EEGInnerSpeechDataset(X_train_split, y_train_split)\n",
    "val_dataset = EEGInnerSpeechDataset(X_val_split, y_validation_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_dataset, val_dataset, train_loader, val_loader\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad705ff",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6286f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.shape: torch.Size([32, 128, 1153])\n",
    "# X_batch.shape: ([16, 4, 128])\n",
    "# y_batch.squeeze().long().shape: torch.Size([32])\n",
    "\n",
    "# for X_batch, y_batch in train_loader:\n",
    "    # print(X_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad4eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on a small batch\n",
    "# X_train_split_small_batch_for_model_testing = X_train_split[:10]\n",
    "# y_train_split_small_batch_for_model_testing = y_train_split[:10]\n",
    "# \n",
    "# train_dataset = InnerSpeechDataset(X_train_split_small_batch_for_model_testing, y_train_split_small_batch_for_model_testing)\n",
    "# val_dataset = InnerSpeechDataset(X_val_split, y_validation_split)\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle = True)\n",
    "# model = InnerSpeechModel()\n",
    "# train_model(model, device, train_loader, epochs=100, model_name=\"InnerSpeechModel_v0\", verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68ec52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EcogClassifier\n",
    "# model = EcogClassifier()\n",
    "\n",
    "# train_model(model, device, train_loader, val_loader, epochs=100, example_input=torch.randn(1, 128, 1153), model_name=\"EcogClassifier_v0\", verbose=False)\n",
    "\n",
    "# EcogClassifier_v0 Epoch 1/100 | Train Loss: 1.394683 | Train Acc: 0.2350 | Val Loss: 1.387976 | Val Acc: 0.2600\n",
    "# Model Checkpoint | epoch: 0 | best_val_loss: 1.3879758723576863\n",
    "# EcogClassifier_v0 Epoch 2/100 | Train Loss: 1.387742 | Train Acc: 0.2450 | Val Loss: 1.386153 | Val Acc: 0.2417\n",
    "# Model Checkpoint | epoch: 1 | best_val_loss: 1.3861531829833984\n",
    "# EcogClassifier_v0 Epoch 3/100 | Train Loss: 1.387694 | Train Acc: 0.2462 | Val Loss: 1.387894 | Val Acc: 0.2533\n",
    "# EcogClassifier_v0 Epoch 4/100 | Train Loss: 1.387689 | Train Acc: 0.2437 | Val Loss: 1.386056 | Val Acc: 0.2467\n",
    "# Model Checkpoint | epoch: 3 | best_val_loss: 1.386056129137675\n",
    "# EcogClassifier_v0 Epoch 5/100 | Train Loss: 1.385896 | Train Acc: 0.2396 | Val Loss: 1.386433 | Val Acc: 0.2600\n",
    "# EcogClassifier_v0 Epoch 6/100 | Train Loss: 1.384991 | Train Acc: 0.2558 | Val Loss: 1.390329 | Val Acc: 0.2467\n",
    "# EcogClassifier_v0 Epoch 7/100 | Train Loss: 1.383838 | Train Acc: 0.2671 | Val Loss: 1.387250 | Val Acc: 0.2467\n",
    "# Early stopping at epoch 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3eec066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG_InnerSpeech_CNN_Classifier_v0 Epoch 1/100 | Train Loss: 1.392717 | Train Acc: 0.2571 | Val Loss: 1.386162 | Val Acc: 0.2483\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 1.3861620553334555\n",
      "EEG_InnerSpeech_CNN_Classifier_v0 Epoch 2/100 | Train Loss: 1.392870 | Train Acc: 0.2612 | Val Loss: 1.392083 | Val Acc: 0.2467\n",
      "EEG_InnerSpeech_CNN_Classifier_v0 Epoch 3/100 | Train Loss: 1.396707 | Train Acc: 0.2437 | Val Loss: 1.395867 | Val Acc: 0.2517\n",
      "EEG_InnerSpeech_CNN_Classifier_v0 Epoch 4/100 | Train Loss: 1.393888 | Train Acc: 0.2642 | Val Loss: 1.412785 | Val Acc: 0.2550\n",
      "EEG_InnerSpeech_CNN_Classifier_v0 Epoch 5/100 | Train Loss: 1.393806 | Train Acc: 0.2533 | Val Loss: 1.388536 | Val Acc: 0.2500\n",
      "EEG_InnerSpeech_CNN_Classifier_v0 Epoch 6/100 | Train Loss: 1.386268 | Train Acc: 0.2662 | Val Loss: 1.532255 | Val Acc: 0.2433\n",
      "Early stopping at epoch 6\n"
     ]
    }
   ],
   "source": [
    "model = EEGInnerSpeechClassifierUpdated()\n",
    "\n",
    "_ = train_model(model, device, train_loader, val_loader, epochs=100, example_input=torch.randn([16, 4, 128]), model_name=\"EEG_InnerSpeech_CNN_Classifier_v0\", verbose=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
