{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73eecad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 23:01:27.317931: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-27 23:01:27.328344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748401287.339246   24966 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748401287.342450   24966 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748401287.351445   24966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748401287.351457   24966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748401287.351458   24966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748401287.351459   24966 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-27 23:01:27.354281: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from glob import glob\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "load_dotenv()\n",
    "root_dir = os.getenv(\"ROOT_DIR\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efede0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class InnerSpeechDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13f8b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN/LSTM hybrid\n",
    "class InnerSpeechModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN component: outputs 256 channels\n",
    "        self.convolv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1),  # Fixed to 128 channels\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.1),\n",
    "            # nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        )\n",
    "\n",
    "        # hidden_size = 32\n",
    "\n",
    "        # Bi-LSTM component (2 Layers)\n",
    "        # self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # self.attn_weight = nn.Linear(2 * hidden_size, 1, bias=False)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Linear(2*hidden_size, 32),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64, 4)  # Matches hidden_size=64\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 128, 1153)\n",
    "        x = self.convolv(x)      # Shape: (batch, 128, 288)\n",
    "        # print(f\"after convolve: {x.shape}\")\n",
    "        # print(f\"x.shape after convolv: {x.shape}\")\n",
    "        # x = x.permute(0, 2, 1)   # Shape: (batch, 288, 128)\n",
    "        # print(f\"after permute: {x.shape}\")\n",
    "        # lstm_out, (h_n, c_n) = self.lstm(x)  # lstm_out shape: (batch, 288, 128)\n",
    "        # print(f\"lstm_out.shape after lstm: {lstm_out.shape}\")\n",
    "        # print(\"\\n\")\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        x = torch.mean(x, dim=2)  # Global average pooling over time\n",
    "\n",
    "        # Compute attention scores\n",
    "        # Flatten across features: attn_score[i, t] = wT * h_{i, t}\n",
    "        # Then softmax over t to get Î±_{i, t}\n",
    "        # attn_scores = self.attn_weight(lstm_out).squeeze(-1)\n",
    "        # attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        # Weighted sum of LSTM outputs:\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
    "\n",
    "        # Classification\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, val_loader=None, epochs=20, model_name=\"model\", example_input=None, checkpoint_dir=\"models/\", verbose=False):\n",
    "    GRAD_CLIP = 1.0\n",
    "    patience = 3  # epochs\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model_optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "    scheduler = ReduceLROnPlateau(model_optimizer, mode=\"min\", factor=0.5, patience=patience)\n",
    "    writer = SummaryWriter(log_dir='runs/' + model_name)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    if example_input is not None:\n",
    "        writer.add_graph(model, example_input.to(device))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).squeeze().long()\n",
    "            if verbose: print(f\"y_batch.shape: {y_batch.shape}\")\n",
    "            model_optimizer.zero_grad()\n",
    "            logits = model(X_batch)  # logits, shape (batch, 4)\n",
    "            if verbose: print(f\"logits.shape: {logits.shape}\")\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            \n",
    "            model_optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            predicted_classes = logits.argmax(dim=1)\n",
    "            all_train_preds.append(predicted_classes.cpu())\n",
    "            all_train_targets.append(y_batch.cpu())\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_preds = torch.cat(all_train_preds).numpy()\n",
    "        train_targets = torch.cat(all_train_targets).numpy()\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", model_optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_val_preds = []\n",
    "            all_val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device).squeeze().long()\n",
    "                    if verbose: print(f\"y_batch.shape: {y_batch.shape}\")\n",
    "                    logits = model(X_batch)\n",
    "                    if verbose: print(f\"logits.shape: {logits.shape}\")\n",
    "                    loss = criterion(logits, y_batch)\n",
    "                    running_val_loss += loss.item() * X_batch.size(0)\n",
    "                    all_val_preds.append(logits.argmax(dim=1).cpu())\n",
    "                    all_val_targets.append(y_batch.cpu())\n",
    "            \n",
    "            avg_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            val_preds = torch.cat(all_val_preds).numpy()\n",
    "            val_targets = torch.cat(all_val_targets).numpy()\n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "            \n",
    "            writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.6f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.6f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if avg_val_loss < best_val_loss - 1e-5:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stop_counter = 0\n",
    "                print(f\"Model Checkpoint | epoch: {epoch} | best_val_loss: {best_val_loss}\")\n",
    "                torch.save(model.state_dict(), checkpoint_dir + model_name + \".pth\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        else:\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.6f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96351a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Neural Network Model\n",
    "# --- CNN Model ---\n",
    "class EcogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Down to (256, 576)\n",
    "            nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Down to (128, 288)\n",
    "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (64, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),      # (64,)\n",
    "            nn.Linear(64, 4)   # 3 output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.net(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15828fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_channels_fit(X, scaler_name_prefix=None):\n",
    "    \"\"\"\n",
    "    This is used for precise per-channel scaling.\n",
    "    This will improve model performance.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): shape (3000, 128, 1153)\n",
    "        scaler_name_prefix (str): prefix to identify the scaler.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: X_scaled training data of zero-mean & unit variance.\n",
    "    \"\"\"\n",
    "    n_samples, n_channels, n_timespoints = X.shape\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    scales = []\n",
    "    for channel in range(n_channels):\n",
    "        scaler = StandardScaler()\n",
    "        X_ch = X[:, channel, :]\n",
    "        X_scaled[:, channel, :] = scaler.fit_transform(X_ch)\n",
    "        scales.append(scaler)\n",
    "    if scaler_name_prefix is not None: \n",
    "        joblib.dump(scales, scaler_name_prefix + \"_scales.pkl\")\n",
    "    return X_scaled\n",
    "\n",
    "def scale_channels_transform(X, scaler_name_prefix = None):\n",
    "    \"\"\"This is used to scale X test with the same scaler that X_train was scaled with.\n",
    "\n",
    "    Args:\n",
    "        X (numpy.ndarray): shape (1000, 128, 1153)\n",
    "        scaler_name_prefix (str): prefix to identify the scaler.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: X_scaled test data of zero-mean & unit variance.\n",
    "    \"\"\"\n",
    "    if scaler_name_prefix is not None:\n",
    "        scalers = joblib.load(f\"{scaler_name_prefix}_scales.pkl\")\n",
    "    n_samples, n_channels, n_timespoints = X.shape\n",
    "    X_scaled = np.zeros_like(X)\n",
    "    for channel in range(n_channels):\n",
    "        X_ch = X[:, channel, :]\n",
    "        X_scaled[:, channel, :] = scalers[channel].transform(X_ch)\n",
    "    return X_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a587fc6",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ee7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(\"data/X_train.pth\")\n",
    "y_train = torch.load(\"data/y_train.pth\")\n",
    "X_test = torch.load(\"data/X_test.pth\")\n",
    "y_test = torch.load(\"data/y_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba6ca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 128, 1153])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c9be8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 200])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decc3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35d7d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "y_train = y_train.cpu().numpy()\n",
    "X_train = X_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221b806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.cpu().numpy()\n",
    "y_test = y_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd01290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 128, 1153)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d519271b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128, 1153)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7a1cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Data with StandardScaler\n",
    "X_train_scaled = scale_channels_fit(X_train, \"X_train\")\n",
    "X_test_scaled = scale_channels_transform(X_test, \"X_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "650f3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1)\n",
    "assert y_train.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6bd405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "146975b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "287beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_val_split, y_train_split, y_validation_split = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dda976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InnerSpeechDataset(X_train_split, y_train_split)\n",
    "val_dataset = InnerSpeechDataset(X_val_split, y_validation_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad705ff",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6286f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.shape: torch.Size([32, 128, 1153])\n",
    "# y_batch.squeeze().long().shape: torch.Size([32])\n",
    "# for X_batch, y_batch in train_loader:\n",
    "#     print(X_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d634bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models/checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"models/runs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad4eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on a small batch\n",
    "# X_train_split_small_batch_for_model_testing = X_train_split[:10]\n",
    "# y_train_split_small_batch_for_model_testing = y_train_split[:10]\n",
    "# \n",
    "# train_dataset = InnerSpeechDataset(X_train_split_small_batch_for_model_testing, y_train_split_small_batch_for_model_testing)\n",
    "# val_dataset = InnerSpeechDataset(X_val_split, y_validation_split)\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle = True)\n",
    "# model = InnerSpeechModel()\n",
    "# train_model(model, device, train_loader, epochs=100, model_name=\"InnerSpeechModel_v0\", verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68ec52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EcogClassifier_v0 Epoch 1/100 | Train Loss: 1.393480 | Train Acc: 0.2479 | Val Loss: 1.389577 | Val Acc: 0.2467\n",
      "Model Checkpoint | epoch: 0 | best_val_loss: 1.3895774030685424\n",
      "EcogClassifier_v0 Epoch 2/100 | Train Loss: 1.387445 | Train Acc: 0.2558 | Val Loss: 1.388865 | Val Acc: 0.2250\n",
      "Model Checkpoint | epoch: 1 | best_val_loss: 1.388865172068278\n",
      "EcogClassifier_v0 Epoch 3/100 | Train Loss: 1.385513 | Train Acc: 0.2621 | Val Loss: 1.388319 | Val Acc: 0.2183\n",
      "Model Checkpoint | epoch: 2 | best_val_loss: 1.3883187913894652\n",
      "EcogClassifier_v0 Epoch 4/100 | Train Loss: 1.384113 | Train Acc: 0.2771 | Val Loss: 1.387918 | Val Acc: 0.2117\n",
      "Model Checkpoint | epoch: 3 | best_val_loss: 1.3879175821940104\n",
      "EcogClassifier_v0 Epoch 5/100 | Train Loss: 1.383188 | Train Acc: 0.2846 | Val Loss: 1.387807 | Val Acc: 0.2200\n",
      "Model Checkpoint | epoch: 4 | best_val_loss: 1.3878072277704874\n",
      "EcogClassifier_v0 Epoch 6/100 | Train Loss: 1.382720 | Train Acc: 0.2958 | Val Loss: 1.387669 | Val Acc: 0.2267\n",
      "Model Checkpoint | epoch: 5 | best_val_loss: 1.3876687558492025\n",
      "EcogClassifier_v0 Epoch 7/100 | Train Loss: 1.381709 | Train Acc: 0.3038 | Val Loss: 1.387411 | Val Acc: 0.2383\n",
      "Model Checkpoint | epoch: 6 | best_val_loss: 1.3874112780888874\n",
      "EcogClassifier_v0 Epoch 8/100 | Train Loss: 1.380920 | Train Acc: 0.3171 | Val Loss: 1.388242 | Val Acc: 0.2150\n",
      "EcogClassifier_v0 Epoch 9/100 | Train Loss: 1.380102 | Train Acc: 0.3137 | Val Loss: 1.387426 | Val Acc: 0.2483\n",
      "EcogClassifier_v0 Epoch 10/100 | Train Loss: 1.378993 | Train Acc: 0.3208 | Val Loss: 1.387321 | Val Acc: 0.2317\n",
      "Model Checkpoint | epoch: 9 | best_val_loss: 1.3873206599553427\n",
      "EcogClassifier_v0 Epoch 11/100 | Train Loss: 1.378623 | Train Acc: 0.3162 | Val Loss: 1.387088 | Val Acc: 0.2100\n",
      "Model Checkpoint | epoch: 10 | best_val_loss: 1.3870880556106568\n",
      "EcogClassifier_v0 Epoch 12/100 | Train Loss: 1.377821 | Train Acc: 0.3417 | Val Loss: 1.387303 | Val Acc: 0.2183\n",
      "EcogClassifier_v0 Epoch 13/100 | Train Loss: 1.377185 | Train Acc: 0.3467 | Val Loss: 1.387472 | Val Acc: 0.2417\n",
      "EcogClassifier_v0 Epoch 14/100 | Train Loss: 1.376190 | Train Acc: 0.3579 | Val Loss: 1.387767 | Val Acc: 0.2300\n",
      "Early stopping at epoch 14\n"
     ]
    }
   ],
   "source": [
    "model = EcogClassifier()\n",
    "\n",
    "train_model(model, device, train_loader, val_loader, epochs=100, example_input=torch.randn(1, 128, 1153), model_name=\"EcogClassifier_v0\", verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
