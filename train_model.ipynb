{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73eecad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from glob import glob\n",
    "import mne\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from scipy.signal import welch, butter, lfilter\n",
    "\n",
    "from preprocess_eeg_signal import butter_bandpass, bandpass_filter, band_power_envelope, multiband_features \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "load_dotenv()\n",
    "root_dir = os.getenv(\"ROOT_DIR\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efede0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class InnerSpeechDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "770e42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "class EEGInnerSpeechDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8228582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGInnerSpeechClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_bands=4, num_channels=128):\n",
    "        super(EEGInnerSpeechClassifier, self).__init__()\n",
    "        # Conv1: Process channels across bands\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_bands, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Conv2: Separable convolution (depthwise + pointwise)\n",
    "        self.conv2_depth = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2, groups=32)\n",
    "        self.conv2_point = nn.Conv1d(32, 64, kernel_size=1, stride=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Conv3: Standard convolution\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(128 * 16, 256)  # 128 channels * 16 (after pooling)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (batch, 4, 128)\n",
    "        x = self.conv1(x)  # (batch, 32, 128)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)  # (batch, 32, 64)\n",
    "        x = self.conv2_depth(x)  # (batch, 32, 64)\n",
    "        x = self.conv2_point(x)  # (batch, 64, 64)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)  # (batch, 64, 32)\n",
    "        x = self.conv3(x)  # (batch, 128, 32)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)  # (batch, 128, 16)k\n",
    "        x = x.view(x.size(0), -1)  # (batch, 128*16)\n",
    "        x = self.relu4(self.fc1(x))  # (batch, 256)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # (batch, 4)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, val_loader=None, epochs=50, batch_size=16, model_name=\"EEGInnerSpeechCNN\", example_input=None, checkpoint_dir=\"models/\", verbose=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    os.makedirs(\"models/checkpoints\", exist_ok=True)\n",
    "    os.makedirs(\"models/runs\", exist_ok=True)\n",
    "    GRAD_CLIP = 1.0\n",
    "    patience = 5  # epochs\n",
    "    warmup_epochs = 3\n",
    "    initial_lr = 1e-5\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model_optimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
    "    scheduler = ReduceLROnPlateau(model_optimizer, mode=\"min\", factor=0.5, patience=patience, min_lr=1e-5)\n",
    "    writer = SummaryWriter(log_dir='runs/' + model_name)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    if example_input is not None:\n",
    "        writer.add_graph(model, example_input.to(device))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Leanring rate warmup\n",
    "        if epoch < warmup_epochs:\n",
    "            lr = initial_lr + (5e-4 - initial_lr) * (epoch / warmup_epochs)\n",
    "            for param_group in model_optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_targets = []\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).squeeze().long()\n",
    "            if verbose: print(f\"y_batch.shape: {y_batch.shape}\")\n",
    "            model_optimizer.zero_grad()\n",
    "            logits = model(X_batch)  # logits, shape (batch, 4)\n",
    "            if verbose: print(f\"logits.shape: {logits.shape}\")\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            \n",
    "            model_optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item() * X_batch.size(0)\n",
    "            \n",
    "            predicted_classes = logits.argmax(dim=1)\n",
    "            all_train_preds.append(predicted_classes.cpu())\n",
    "            all_train_targets.append(y_batch.cpu())\n",
    "        \n",
    "        avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        train_preds = torch.cat(all_train_preds).numpy()\n",
    "        train_targets = torch.cat(all_train_targets).numpy()\n",
    "        train_acc = accuracy_score(train_targets, train_preds)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", model_optimizer.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            all_val_preds = []\n",
    "            all_val_targets = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device).squeeze().long()\n",
    "                    if verbose: print(f\"y_batch.shape: {y_batch.shape}\")\n",
    "                    logits = model(X_batch)\n",
    "                    if verbose: print(f\"logits.shape: {logits.shape}\")\n",
    "                    loss = criterion(logits, y_batch)\n",
    "                    running_val_loss += loss.item() * X_batch.size(0)\n",
    "                    all_val_preds.append(logits.argmax(dim=1).cpu())\n",
    "                    all_val_targets.append(y_batch.cpu())\n",
    "            \n",
    "            avg_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "            val_preds = torch.cat(all_val_preds).numpy()\n",
    "            val_targets = torch.cat(all_val_targets).numpy()\n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "            \n",
    "            writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "            writer.add_scalar(\"Accuracy/Validation\", val_acc, epoch)\n",
    "\n",
    "            if epoch >= warmup_epochs:\n",
    "                scheduler.step(avg_val_loss)\n",
    "\n",
    "            print(f\"{model_name} Epoch {epoch+1}/{epochs} | \"\n",
    "                  f\"Train Loss: {avg_train_loss:.6f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {avg_val_loss:.6f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Save best model checkpoint\n",
    "            if avg_val_loss < best_val_loss - 1e-5:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stop_counter = 0\n",
    "                print(f\"Model Checkpoint | epoch: {epoch} | best_val_loss: {best_val_loss}\")\n",
    "                torch.save(model.state_dict(), checkpoint_dir + model_name + \".pth\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                if early_stop_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96351a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convolutional Neural Network Model\n",
    "# --- CNN Model ---\n",
    "class EcogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Down to (256, 576)\n",
    "            nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),  # Down to (128, 288)\n",
    "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),  # Output: (64, 1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),      # (64,)\n",
    "            nn.Linear(64, 4)   # 3 output classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.net(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db194545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3f8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data, lowcut, highcut, fs=256.0):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs)\n",
    "    filtered = lfilter(b, a, data, axis=2)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3339cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(data, fs=256.0, bands=[(0.5, 4), (4, 8), (8, 12), (12, 30)], batch_size=100):\n",
    "    num_trials, num_channels, num_samples = data.shape\n",
    "    num_bands = len(bands)\n",
    "    psd_features = np.zeros((num_trials, num_bands, num_channels), dtype=np.float32)\n",
    "\n",
    "    for start_idx in range(0, num_trials, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_trials)\n",
    "        batch_data = data[start_idx:end_idx]\n",
    "\n",
    "        for band_idx, (low, high) in enumerate(bands):\n",
    "            filtered = bandpass_filter(batch_data, low, high, fs)\n",
    "            freqs, psd = welch(filtered, fs=fs, axis=2, nperseg=256)\n",
    "            psd_mean = np.mean(psd, axis=2)\n",
    "            psd_features[start_idx:end_idx, band_idx, :] = psd_mean\n",
    "            del filtered, psd, psd_mean\n",
    "    \n",
    "        del batch_data\n",
    "    return psd_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d825da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_channels_fit(X, name):\n",
    "    scaler = RobustScaler()\n",
    "    X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "    scaler.fit(X_reshaped)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635ffd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_channels_transform(X, scaler):\n",
    "    X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "    X_scaled = scaler.transform(X_reshaped)\n",
    "    return X_scaled.reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691fdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGInnerSpeechClassifierUpdated(nn.Module):\n",
    "    def __init__(self, num_classes=4, num_bands=4):\n",
    "        super(EEGInnerSpeechClassifierUpdated, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(num_bands, 32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.depthwise = nn.Conv1d(32, 32, kernel_size=5, padding=2, groups=32)\n",
    "        self.pointwise = nn.Conv1d(32, 64, kernel_size=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(self.relu2(self.bn2(self.pointwise(self.depthwise(x)))))\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a587fc6",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68ee7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(\"data/X_train.pth\")\n",
    "y_train = torch.load(\"data/y_train.pth\")\n",
    "# X_test = torch.load(\"data/X_test.pth\")\n",
    "# y_test = torch.load(\"data/y_test.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "400f0522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the raw strings as integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = torch.tensor(label_encoder.fit_transform(y_train), device=device)  # integer labels: 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5809328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b3522ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3200, 128, 640])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d97a4ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0,  ..., 1, 2, 0], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "decc3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.long()\n",
    "# y_test = y_test.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "35d7d27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets\n",
    "y_train = y_train.cpu().numpy()\n",
    "X_train = X_train.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "10b20f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3200, 128, 640)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a80b3b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.71176738e-06,  1.16846070e-06,  3.31827414e-06, ...,\n",
       "          1.45639676e-05,  1.36433307e-05,  9.70088202e-06],\n",
       "        [ 1.59351507e-06,  3.84599622e-07,  3.03085951e-06, ...,\n",
       "          1.46348033e-05,  1.18493852e-05,  8.63002326e-06],\n",
       "        [ 2.50736617e-06,  1.45015636e-06,  4.11861471e-06, ...,\n",
       "          1.63883400e-05,  1.36700965e-05,  1.03945050e-05],\n",
       "        ...,\n",
       "        [ 8.63118801e-07, -9.67614975e-07,  7.31853769e-06, ...,\n",
       "          3.82829762e-06,  1.74064473e-06,  3.74019630e-06],\n",
       "        [ 1.06878054e-06, -7.16682192e-07,  7.37907106e-06, ...,\n",
       "          3.53623059e-06,  1.96710643e-06,  3.74723605e-06],\n",
       "        [ 7.90739272e-07,  3.03287683e-07,  8.61955922e-06, ...,\n",
       "          1.08448432e-05,  8.78994442e-06,  7.95353232e-06]],\n",
       "\n",
       "       [[-1.19148411e-05, -1.36932596e-05, -1.51152241e-05, ...,\n",
       "          7.38314402e-06,  8.36708480e-06,  7.61167036e-06],\n",
       "        [-1.37663030e-05, -1.48070165e-05, -1.48490914e-05, ...,\n",
       "          6.93859585e-06,  8.07943282e-06,  6.55539202e-06],\n",
       "        [-1.32503265e-05, -1.41286153e-05, -1.40318011e-05, ...,\n",
       "          7.26237339e-06,  7.76590770e-06,  7.90157263e-06],\n",
       "        ...,\n",
       "        [-7.28190221e-06, -7.71677465e-06, -6.16311243e-06, ...,\n",
       "          5.73483093e-06,  4.41665950e-06,  6.63815442e-06],\n",
       "        [-1.17347352e-05, -1.08046793e-05, -9.27743869e-06, ...,\n",
       "          2.72164851e-06,  2.60274809e-06,  5.20053289e-06],\n",
       "        [-1.10625890e-05, -1.00001278e-05, -8.29519856e-06, ...,\n",
       "          4.13846213e-06,  3.67055059e-06,  6.17591470e-06]],\n",
       "\n",
       "       [[ 1.76600687e-07,  1.60838618e-06, -5.96167251e-07, ...,\n",
       "         -7.62020234e-06, -1.11462455e-05, -1.22320263e-05],\n",
       "        [ 3.78652757e-07,  1.75176551e-06, -8.79203440e-08, ...,\n",
       "         -8.34283234e-06, -1.14424623e-05, -1.15071249e-05],\n",
       "        [-6.29202827e-07,  8.23402679e-07, -1.90225614e-06, ...,\n",
       "         -8.07905420e-06, -1.03810289e-05, -1.28545268e-05],\n",
       "        ...,\n",
       "        [-1.75901260e-05, -1.51822285e-05, -1.75247791e-05, ...,\n",
       "         -8.25922931e-07, -9.07590818e-07, -2.08614221e-06],\n",
       "        [-6.72891298e-06, -8.11539914e-06, -7.53275573e-06, ...,\n",
       "          6.06047328e-08,  8.63334581e-07,  6.48723023e-06],\n",
       "        [-8.97017688e-06, -5.67598965e-06, -6.70786060e-06, ...,\n",
       "         -1.18081887e-06, -1.67451082e-06, -2.10717032e-06]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.87145488e-05,  1.56697079e-05,  9.98572610e-06, ...,\n",
       "         -8.65342095e-06, -1.26098541e-05, -7.82280552e-06],\n",
       "        [ 1.87682293e-05,  1.71856895e-05,  1.00974211e-05, ...,\n",
       "         -9.32574162e-06, -1.25045648e-05, -8.19917279e-06],\n",
       "        [ 1.80966168e-05,  1.63579154e-05,  9.73422370e-06, ...,\n",
       "         -9.86772184e-06, -1.40346873e-05, -9.30714154e-06],\n",
       "        ...,\n",
       "        [ 2.66866528e-05,  2.65365207e-05,  2.75734228e-06, ...,\n",
       "         -1.93680318e-05, -2.04358457e-05, -1.53543960e-05],\n",
       "        [ 2.46574734e-05,  2.51136285e-05,  2.03005618e-06, ...,\n",
       "         -1.96771968e-05, -1.97113329e-05, -1.57202695e-05],\n",
       "        [ 2.60367175e-05,  2.51939355e-05,  2.30051037e-06, ...,\n",
       "         -1.94563230e-05, -2.04308078e-05, -1.65064310e-05]],\n",
       "\n",
       "       [[-3.14795514e-06,  7.18008344e-07,  5.71885053e-06, ...,\n",
       "         -4.11447925e-06, -8.55894961e-06, -4.50833403e-06],\n",
       "        [-3.03475848e-06,  3.21071735e-07,  4.94176408e-06, ...,\n",
       "         -5.09421616e-06, -8.23520769e-06, -5.14058661e-06],\n",
       "        [-2.38756165e-06,  4.31595018e-07,  5.09761948e-06, ...,\n",
       "         -5.04929664e-06, -8.07172459e-06, -5.03649688e-06],\n",
       "        ...,\n",
       "        [-1.98070348e-05, -1.50310472e-05, -1.35864459e-05, ...,\n",
       "          1.01201305e-05,  9.37728080e-06,  1.86228232e-05],\n",
       "        [-5.72628790e-06,  1.51630968e-06,  2.33055888e-06, ...,\n",
       "          4.34535142e-06,  5.40358139e-06,  1.36303764e-05],\n",
       "        [-5.01510486e-06,  8.22584137e-07,  3.79295198e-06, ...,\n",
       "          4.27005101e-06,  5.71537056e-06,  1.46598711e-05]],\n",
       "\n",
       "       [[ 9.90178838e-06,  5.14996763e-06,  3.89218425e-06, ...,\n",
       "          3.79432355e-06, -1.88837011e-06, -7.63169407e-06],\n",
       "        [ 1.07126434e-05,  6.29535540e-06,  3.77100736e-06, ...,\n",
       "          5.27200896e-06, -1.46535766e-07, -6.88487951e-06],\n",
       "        [ 1.01205203e-05,  7.22861240e-06,  4.93943696e-06, ...,\n",
       "          6.32674466e-06, -6.86567175e-07, -6.97480332e-06],\n",
       "        ...,\n",
       "        [ 1.97109812e-05,  1.63639007e-05,  1.76300892e-05, ...,\n",
       "          1.09145557e-05,  4.71547384e-06,  9.49861141e-06],\n",
       "        [ 1.65112833e-05,  1.34831801e-05,  1.60698858e-05, ...,\n",
       "          1.55584804e-05,  4.15861990e-06,  7.94871115e-06],\n",
       "        [ 1.70251427e-05,  1.50841667e-05,  1.86952743e-05, ...,\n",
       "          1.65284295e-05,  5.44416510e-06,  8.75165401e-06]]],\n",
       "      shape=(3200, 128, 640))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345fa37",
   "metadata": {},
   "source": [
    "## Transforming X data into power spectral frequency bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = X_test.cpu().numpy()\n",
    "# y_test = y_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eabd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = 256.0\n",
    "# X_train_psd = compute_psd(X_train, fs=fs, batch_size=100)\n",
    "\n",
    "# Per-channel Scaling\n",
    "\n",
    "# scaler = scale_channels_fit(X_train_psd, \"X_train\")\n",
    "# X_train_scaled = scale_channels_transform(X_train_psd, scaler)\n",
    "# del X_train, \n",
    "# # X_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400dde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "650f3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1)\n",
    "assert y_train.shape[0] == X_train_scaled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "287beacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_split, X_val_split, y_train_split, y_validation_split = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e36d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dda976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EEGInnerSpeechDataset(X_train_split, y_train_split)\n",
    "val_dataset = EEGInnerSpeechDataset(X_val_split, y_validation_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del train_dataset, val_dataset, train_loader, val_loader\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad705ff",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6286f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batch.shape: torch.Size([32, 128, 1153])\n",
    "# X_batch.shape: ([16, 4, 128])\n",
    "# y_batch.squeeze().long().shape: torch.Size([32])\n",
    "\n",
    "# for X_batch, y_batch in train_loader:\n",
    "    # print(X_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4eed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on a small batch\n",
    "# X_train_split_small_batch_for_model_testing = X_train_split[:10]\n",
    "# y_train_split_small_batch_for_model_testing = y_train_split[:10]\n",
    "# \n",
    "# train_dataset = InnerSpeechDataset(X_train_split_small_batch_for_model_testing, y_train_split_small_batch_for_model_testing)\n",
    "# val_dataset = InnerSpeechDataset(X_val_split, y_validation_split)\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle = True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle = True)\n",
    "# model = InnerSpeechModel()\n",
    "# train_model(model, device, train_loader, epochs=100, model_name=\"InnerSpeechModel_v0\", verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EcogClassifier\n",
    "# model = EcogClassifier()\n",
    "\n",
    "# train_model(model, device, train_loader, val_loader, epochs=100, example_input=torch.randn(1, 128, 1153), model_name=\"EcogClassifier_v0\", verbose=False)\n",
    "\n",
    "# EcogClassifier_v0 Epoch 1/100 | Train Loss: 1.394683 | Train Acc: 0.2350 | Val Loss: 1.387976 | Val Acc: 0.2600\n",
    "# Model Checkpoint | epoch: 0 | best_val_loss: 1.3879758723576863\n",
    "# EcogClassifier_v0 Epoch 2/100 | Train Loss: 1.387742 | Train Acc: 0.2450 | Val Loss: 1.386153 | Val Acc: 0.2417\n",
    "# Model Checkpoint | epoch: 1 | best_val_loss: 1.3861531829833984\n",
    "# EcogClassifier_v0 Epoch 3/100 | Train Loss: 1.387694 | Train Acc: 0.2462 | Val Loss: 1.387894 | Val Acc: 0.2533\n",
    "# EcogClassifier_v0 Epoch 4/100 | Train Loss: 1.387689 | Train Acc: 0.2437 | Val Loss: 1.386056 | Val Acc: 0.2467\n",
    "# Model Checkpoint | epoch: 3 | best_val_loss: 1.386056129137675\n",
    "# EcogClassifier_v0 Epoch 5/100 | Train Loss: 1.385896 | Train Acc: 0.2396 | Val Loss: 1.386433 | Val Acc: 0.2600\n",
    "# EcogClassifier_v0 Epoch 6/100 | Train Loss: 1.384991 | Train Acc: 0.2558 | Val Loss: 1.390329 | Val Acc: 0.2467\n",
    "# EcogClassifier_v0 Epoch 7/100 | Train Loss: 1.383838 | Train Acc: 0.2671 | Val Loss: 1.387250 | Val Acc: 0.2467\n",
    "# Early stopping at epoch 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EEGInnerSpeechClassifierUpdated()\n",
    "\n",
    "_ = train_model(model, device, train_loader, val_loader, epochs=100, example_input=torch.randn([16, 4, 128]), model_name=\"EEG_InnerSpeech_CNN_Classifier_v0\", verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30924efd",
   "metadata": {},
   "source": [
    "### Validating the model will function: Overfitting on a small batch of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fdfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to overfit a small batch\n",
    "small_X = torch.tensor(X_train_scaled[:32], dtype=torch.float32).to(device)\n",
    "small_y = torch.tensor(y_train[:32], dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "X_train_split, X_val_split, y_train_split, y_validation_split = train_test_split(\n",
    "    X_train_scaled[:32], y_train[:32], test_size=0.2, random_state=42, stratify=y_train[:32]\n",
    ")\n",
    "\n",
    "train_dataset = EEGInnerSpeechDataset(small_X, small_y)\n",
    "val_dataset = EEGInnerSpeechDataset(X_val_split, y_validation_split)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle = False)\n",
    "\n",
    "model = EEGInnerSpeechClassifierUpdated().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    logits = model(small_X)\n",
    "    loss = loss_fn(logits, small_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    preds = logits.argmax(dim=1)\n",
    "    acc = (preds == small_y).float().mean().item()\n",
    "    print(f\"Epoch {i}: Loss = {loss.item():.4f} | Acc = {acc:.2f}\")\n",
    "    if acc == 1.0:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
